{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d24cade-542e-40fe-81b8-32490551abf8",
   "metadata": {},
   "source": [
    "## Visualize attention patterns\n",
    "\n",
    "How attention patterns evolve from early to late processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098d40df-7d67-48f0-a0d8-1b3e84a61a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a532083b-e22e-4994-aa51-58a8a4bf7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofism/miniconda3/envs/llava-lens/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025ea2a8bbf34ed39cb25512c4cce619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `use_fast=True` but `torchvision` is not available. Falling back to the slow image processor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f990d437ebe49b1ab69ebe88635360e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llava import LLaVA\n",
    "llava = LLaVA()\n",
    "llava.load_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93c608a-1556-4474-b8de-eb57add120ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_to_visualize = [1, 6, 11, 16, 21, 26, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3d4ab99-ec04-49b4-8833-ad427374b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, question, image_name in llava.dataset:\n",
    "    inputs_embeds, _, _, _, _ = llava.get_llm_input_embeddings(image, question)\n",
    "    logits, cache = llava.hooked_llm.run_with_cache(inputs_embeds, start_at_layer=0, remove_batch_dim=True)\n",
    "    for layer_to_visualize in layers_to_visualize:\n",
    "        tokens_to_show = 40\n",
    "        \n",
    "        attention_pattern = cache[\"pattern\", layer_to_visualize, \"attn\"]\n",
    "        \n",
    "        product = inputs_embeds @ llava.language_model.model.embed_tokens.weight.T.to(llava.device)  \n",
    "        llama_str_tokens = llava.hooked_llm.to_str_tokens(product.argmax(dim=-1)[0])\n",
    "        \n",
    "        attention_html = cv.attention.attention_patterns(tokens=llama_str_tokens[-tokens_to_show:], \n",
    "                                                attention=attention_pattern[:, -tokens_to_show:, -tokens_to_show:])\n",
    "        # display(attention_html)\n",
    "        with open(llava.results_dir / f\"{image_name}_{tokens_to_show}_{layer_to_visualize}_attention.html\", 'w') as f:\n",
    "            f.write(attention_html._repr_html_())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-lens",
   "language": "python",
   "name": "llava-lens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
