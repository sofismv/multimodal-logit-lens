{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e915d2bd-a359-4087-8839-a250c45f51d8",
   "metadata": {},
   "source": [
    "## Attention when generating analysis\n",
    "\n",
    "What are attention patterns when generating \"cat\" or \"dog\n",
    "\n",
    "Same notebook as generate_tokens.ipynb and visualize_attention.ipynb\n",
    "\n",
    "HTMLs are saved in analysis_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3aefd7e-d7a8-4141-855b-33ff34a09615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofism/miniconda3/envs/llava-lens/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import circuitsvis as cv\n",
    "\n",
    "from llava import LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddb66b3-d79f-49fc-82ab-9c0e28844245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c8c9ee196b47aa9648c22885662fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `use_fast=True` but `torchvision` is not available. Falling back to the slow image processor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e9e84bd2144eeb6e976e7d58459a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading 1 image\n",
    "llava = LLaVA()\n",
    "llava.load_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffb14a0-a45f-4d7d-9a99-046d38735dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_new_tokens = 10  # max number of tokens to generate\n",
    "top_k = 5            # number of top tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af14baf1-e7e7-477b-9393-451baa0da4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_token(llava, i, current_inputs_embeds, top_k, image_name):\n",
    "    # get the unembedding matrix\n",
    "    unembed = llava.hooked_llm.W_U.to(dtype=torch.float32)\n",
    "    \n",
    "    # run with cache\n",
    "    logits, cache = llava.hooked_llm.run_with_cache(\n",
    "        current_inputs_embeds, \n",
    "        start_at_layer=0, \n",
    "        remove_batch_dim=True\n",
    "    )\n",
    "\n",
    "    layers_to_visualize = [1, 6, 11, 16, 21, 26, 31]\n",
    "    for layer_to_visualize in layers_to_visualize:\n",
    "        tokens_to_show = 40\n",
    "        \n",
    "        attention_pattern = cache[\"pattern\", layer_to_visualize, \"attn\"]\n",
    "        \n",
    "        product = current_inputs_embeds @ llava.language_model.model.embed_tokens.weight.T.to(llava.device)  \n",
    "        llama_str_tokens = llava.hooked_llm.to_str_tokens(product.argmax(dim=-1)[0])\n",
    "        \n",
    "        attention_html = cv.attention.attention_patterns(tokens=llama_str_tokens[-tokens_to_show:], \n",
    "                                                attention=attention_pattern[:, -tokens_to_show:, -tokens_to_show:])\n",
    "        with open(llava.results_dir / f\"{image_name}_{i}_{layer_to_visualize}_attention.html\", 'w') as f:\n",
    "            f.write(attention_html._repr_html_())\n",
    "\n",
    "    # get residual stream across all layers\n",
    "    resid_stack, labels = cache.accumulated_resid(layer=-1, incl_mid=True, return_labels=True)\n",
    "    \n",
    "    # apply layer normalization to the residual stream\n",
    "    resid_stack_ln = cache.apply_ln_to_stack(resid_stack, layer=-1, pos_slice=-1)\n",
    "    \n",
    "    # compute token logits for each layer\n",
    "    token_logits = resid_stack_ln @ unembed\n",
    "    \n",
    "    # get top-k predicted tokens for each layer and position\n",
    "    _, topk_indices = torch.topk(token_logits, k=top_k, dim=-1)\n",
    "\n",
    "    # decode\n",
    "    step_top_tokens = [[\n",
    "        [llava.tokenizer.decode([idx]) for idx in topk_indices[layer, pos]]\n",
    "        for pos in range(topk_indices.shape[1])\n",
    "    ] for layer in range(topk_indices.shape[0])]\n",
    "    \n",
    "    # get the final model prediction for next token\n",
    "    next_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # select the token with highest probability\n",
    "    next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "    \n",
    "    # get embedding\n",
    "    next_token_embed = llava.hooked_llm.W_E[next_token_id]\n",
    "    \n",
    "    return step_top_tokens, next_token_id, next_token_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38517b37-4dd0-4e3d-b179-95682102cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tokens(llava, inputs_embeds, max_new_tokens, top_k, image_name):\n",
    "    generation_analysis = []\n",
    "    generated_token_ids = []\n",
    "    current_inputs_embeds = inputs_embeds\n",
    "    \n",
    "    for i in range(max_new_tokens):\n",
    "        print(f\"step = {i+1}\")\n",
    "        \n",
    "        step_top_tokens, next_token_id, next_token_embed = generate_single_token(\n",
    "            llava, i, current_inputs_embeds, top_k, image_name\n",
    "        )\n",
    "        \n",
    "        generation_analysis.append(step_top_tokens)\n",
    "        \n",
    "        generated_token_ids.append(next_token_id.item())\n",
    "        \n",
    "        generated_token_str = llava.tokenizer.decode([next_token_id])\n",
    "        print(f\"token: '{generated_token_str}' (id: {next_token_id.item()})\")\n",
    "        \n",
    "        new_token_embed_3d = next_token_embed.unsqueeze(0).unsqueeze(0)\n",
    "        current_inputs_embeds = torch.cat(\n",
    "            [current_inputs_embeds, new_token_embed_3d],\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # check for end-of-sequence token\n",
    "        if next_token_id.item() == 2:\n",
    "            break\n",
    "    \n",
    "    return generation_analysis, generated_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06124e84-940c-443d-83b6-a45fb4e3ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_visualization_data(generation_analysis, generated_token_ids, inputs_embeds, llava):\n",
    "    num_layers = len(generation_analysis[0])\n",
    "    prompt_len = inputs_embeds.shape[1]\n",
    "    \n",
    "    token_grid = []\n",
    "    match_grid = []\n",
    "    \n",
    "    for i, step_analysis in enumerate(generation_analysis):\n",
    "        # get the actual generated token\n",
    "        final_generated_token_str = llava.tokenizer.decode([generated_token_ids[i]]).strip()\n",
    "        \n",
    "        # calculate the position index for the current sequence length\n",
    "        current_seq_len = prompt_len + i\n",
    "        final_pos_idx = current_seq_len - 1\n",
    "    \n",
    "        # extract predictions from each layer for the final position\n",
    "        step_tokens = []\n",
    "        match_row = []\n",
    "        for layer_idx in range(num_layers):\n",
    "            # get the top prediction from this layer\n",
    "            token_str = step_analysis[layer_idx][final_pos_idx][0].strip()\n",
    "            step_tokens.append(token_str)\n",
    "            \n",
    "            # check if this layer's prediction matches the final generated token\n",
    "            match_row.append(token_str == final_generated_token_str)\n",
    "            \n",
    "        token_grid.append(step_tokens)\n",
    "        match_grid.append(match_row)\n",
    "    \n",
    "    token_grid = np.array(token_grid).T\n",
    "    match_grid = np.array(match_grid).T\n",
    "    \n",
    "    # plotting every 2nd layer\n",
    "    layer_indices = np.arange(0, num_layers, 2)\n",
    "    token_grid = token_grid[layer_indices, :]\n",
    "    match_grid = match_grid[layer_indices, :]\n",
    "    \n",
    "    return token_grid, match_grid, layer_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4abacf4-1210-43bc-bc6a-7942cb2fe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization(token_grid, match_grid, layer_indices, question, image_name, llava):\n",
    "    num_steps = token_grid.shape[1]\n",
    "    \n",
    "    colors = np.where(match_grid, '#FFD700', '#E0E0E0')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    for y in range(token_grid.shape[0]):\n",
    "        for x in range(token_grid.shape[1]):\n",
    "            ax.add_patch(plt.Rectangle((x - 0.5, y - 0.5), 1, 1, color=colors[y, x]))\n",
    "            \n",
    "            text_color = 'black'\n",
    "            ax.text(\n",
    "                x, y,\n",
    "                f\"'{token_grid[y, x]}'\",\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                fontsize=8,\n",
    "                color=text_color,\n",
    "                fontweight='bold' if match_grid[y, x] else 'normal'\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel(\"Generation step\", fontsize=12)\n",
    "    ax.set_ylabel(\"Layer\", fontsize=12)\n",
    "    ax.set_title(f\"Predicted Tokens for question: {question}\", fontsize=14, pad=10)\n",
    "    \n",
    "    ax.set_xticks(np.arange(num_steps))\n",
    "    ax.set_yticks(np.arange(len(layer_indices)))\n",
    "    ax.set_yticklabels([f\"Layer {i}\" for i in layer_indices])\n",
    "    \n",
    "    ax.set_xlim(-0.5, num_steps - 0.5)\n",
    "    ax.set_ylim(len(layer_indices) - 0.5, -0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(llava.results_dir / f\"{image_name}_token_generation.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03dca48-e196-4207-8fa6-e17745974985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample(llava, image, question, image_name, max_new_tokens, top_k):\n",
    "    inputs_embeds, _, _, _, _ = llava.get_llm_input_embeddings(image, question)\n",
    "    \n",
    "    generation_analysis, generated_token_ids = generate_tokens(\n",
    "        llava, inputs_embeds, max_new_tokens, top_k, image_name\n",
    "    )\n",
    "    \n",
    "    full_generated_text = llava.tokenizer.decode(generated_token_ids)\n",
    "    print(f\"full text: {full_generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5ff4eb-4672-4c14-b393-04537548cec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 1\n",
      "token: 'The' (id: 450)\n",
      "step = 2\n",
      "token: 'cat' (id: 6635)\n",
      "step = 3\n",
      "token: 'is' (id: 338)\n",
      "step = 4\n",
      "token: 'black' (id: 4628)\n",
      "step = 5\n",
      "token: '.' (id: 29889)\n",
      "step = 6\n",
      "token: '</s>' (id: 2)\n",
      "full text: The cat is black.</s>\n",
      "step = 1\n",
      "token: 'The' (id: 450)\n",
      "step = 2\n",
      "token: 'dog' (id: 11203)\n",
      "step = 3\n",
      "token: 'is' (id: 338)\n",
      "step = 4\n",
      "token: 'black' (id: 4628)\n",
      "step = 5\n",
      "token: 'and' (id: 322)\n",
      "step = 6\n",
      "token: 'white' (id: 4796)\n",
      "step = 7\n",
      "token: '.' (id: 29889)\n",
      "step = 8\n",
      "token: '</s>' (id: 2)\n",
      "full text: The dog is black and white.</s>\n"
     ]
    }
   ],
   "source": [
    "for image, question, image_name in llava.dataset:\n",
    "    analyze_sample(llava, image, question, image_name, max_new_tokens, top_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-lens",
   "language": "python",
   "name": "llava-lens"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
